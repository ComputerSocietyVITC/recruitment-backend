# Fluentd configuration for recruitment backend logging
# This config processes structured JSON logs and routes them to Elasticsearch

<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Parse JSON logs from recruitment backend
<filter recruitment.backend>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</filter>

# Parse JSON logs from postgres
<filter recruitment.postgres>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    @type regexp
    expression /^(?<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3} \w+) \[(?<pid>\d+)\] (?<level>\w+): (?<message>.*)/
    time_key timestamp
    time_format %Y-%m-%d %H:%M:%S.%L %Z
  </parse>
</filter>

# Add common metadata to all logs
<filter recruitment.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['ENV'] || 'production'}"
    log_source fluentd
    timestamp_processed ${time}
  </record>
</filter>

# Route logs to Elasticsearch
<match recruitment.**>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix recruitment
    logstash_dateformat %Y.%m.%d
    include_tag_key true
    tag_key service_tag
    type_name _doc
  </store>
</match>

# Fallback for any unmatched logs
<match **>
  @type stdout
</match>
